---
title: 베이지언 심층학습 회귀모형의 신뢰구간 보정
category: machine-learning
tags:
  - machine-learning
  - bayesian
created_at: 2024-04-19 08:28:43 +09:00
last_modified_at: 2024-04-19 14:49:14 +09:00
excerpt: 베이지언 심층학습 회귀모델의 신뢰구간을 실제 관측치와 결맞게 보정하는 방법.
---

https://richardcsuwandi.medium.com/gaussian-process-regression-using-gpytorch-2c174286f9cc

https://bayesian-neural-network-pytorch.readthedocs.io/en/latest/

## 베이지언 심층학습 회귀모형

**베이지언 심층학습 회귀모형**은 입력 벡터가 주어졌을 때 출력으로 고정된 스칼라 값 대신 출력이 가질 수 있는 **분포**를 반환하도록 학습된 심층학습 모델입니다.  만일 출력이 정규분포를 이룰 것으로 가정한다면, 이것은 결국 주어진 입력 벡터 $x$에 대해 출력 변수 $y$가 평균과 표준편차로 대변되는 확률변수로 주어진다는 의미가 될 것입니다.

즉, **분포**를 반환한다고는 하지만, 사실상 길이 2인 벡터 $(m_x, \sigma_x)$를 반환하는 모델이 된다고 보아도 좋을 것입니다.

## 회귀모형의 신뢰구간

이제 어떤 베이지언 심층학습 회귀 예측기 $H$가 있어 입력 $x$에 대해 정규분포 $(m_x, \sigma_x)$를 따르는 출력 $y$를 내놓는다고 합시다.  그러면 이 정규분포를 이용해 **신뢰구간**을 계산하는 것 역시 자연스러운 수순일 것입니다.

## 잘 교정된 예측기

Kuleshov, Fenner, Ermon는 2018년 발표한 논문[^1]에서 어떤 특정한 기대를 충족하는 예측기 $H$에 대해 $H$가 잘 **교정되었다(calibrated)**고 정의하였습니다.

[^1]: [V. Kuleshove, N. Fenner, and S. Ermon, *Accurate Uncertainties for Deep Learning Using Calibrated Regression*, arXiv](arxiv.org/pdf/1807.00263.pdf)

데이터 집합 $\mathcal{S}=\lbrace{(x_t, y_t)}\rbrace _{t=1}^T$상에서 학습된 **잘 교정된 예측기** $H$가 만족하는 성질을 수식으로 나타내면 아래와 같습니다.

$$\frac{\sum _{t=1}^{T}\Bbb{I}\lbrace y_t\le F_t^{-1}(p)\rbrace}{T}\rightarrow p\quad\forall p \in \lbrack0,1\rbrack\tag{1}$$

이 때 $F_t^{-1}(p)$는 예측기가 추정한 분포 $H(x_t)\sim N(m_{x_t}, \sigma_{x_t})$ 에서 구간 $\lbrack -\infty, y\rbrack$에 대한 적분이 $p$가 되도록 하는 $y$의 값입니다.

그림 1. $y$와 $y_t$

위의 조건은 보다 일반적인 경우에 적용될 수 있는 느슨한 조건입니다.  우리가 일반적으로 정규분포와 관련지어 생각하는 신뢰구간의 관점에서는 $F_t^{-1}(p)$를 계산하는 방법이 재정의되어야 할 텐데, 구체적으로는 적분을 계산하는 구간을 $\lbrack -\infty, y\rbrack$에서 $\lbrack m_x-y, m_x+y\rbrack$로 변경해야 할 것입니다.

하지만 위와 같이 변경된 구간에 대한 조건은 **식 1**의 조건이 충족되면 자동적으로 만족될 것이라는 점이 자명합니다.  따라서, **식 1**에 대해서만 생각하는 것으로도 우리가 원하는 케이스들로 일반화할 수 있고, 잘 교정된 예측기에 대한 Kuleshov 등의 논의는 **식 1**의 표기를 기준으로 전개됩니다.

## 잘 교정된 예측기 - 농부의 기대

**식 1**의 의미를 설명하기 위해 하나의 예시를 생각해보겠습니다.  우리는 데이터와 머신러닝을 제법 잘 다루는 농부인데, 강수량과 일조량 등 농사와 관련하여 중요한 몇 가지 변수들을 입력 벡터로 하여 수확량을 예측하는 베이지언 심층학습 회귀 예측기 $H$를 가지고 있다고 해 봅시다.

이 모델이 어느 해의 수확량 분포를 $N(m_x, \sigma_x)$으로 예측했다고 할 때, 그 해의 실제 수확량 $y$가 정확히 $m_x$와 일치한다면 아주 행복하겠지만, 일반적으로는 그렇지 않을 것입니다.  하지만 단일 스칼라 출력만을 내놓는 일반적인 회귀 모델과 달리 여러분의 모델은 수확량의 분포를 제공할 능력이 있고, 이것을 이용하면 적당한 확률 $p$에 대한 신뢰구간을 생각할 수 있습니다.

이것은 여러분이 일반 회귀 모델을 사용할 때와 달리, 예컨대 $p$=0.9에 대한 신뢰구간을 그려 놓은 다음 **'올해 수확량이 이 범위 안에 들어올 확률이 90%는 되지 않을까?'** 따위의 생각을 해 볼 수 있다는 뜻입니다.  정확히는, 이 예측기를 통해 충분히 많은 수의 예측을 시도하고 그 결과를 확인해 본다면, 시도 횟수 중 90%의 비율 정도는 실제 수확량이 $p$=0.9에 대한 신뢰구간 내에서 발견될 것이라는 기대를 가질 수 있을 것입니다.

**신뢰구간의 정의**<br>지금의 논의에서 확률 $p$에 대한 **신뢰구간**이라 함은 평균을 기준으로 대칭되는 구간이 아닌, Kuleshov 등의 논의를 따라 아래로 열린 구간 $\lbrack -\infty, F_t^{-1}(p)\rbrack$에 대해 논의하는 것임에 유의합시다.
{: .notice--warning}

물론 $H$ 또한 그저 모델에 불과할 뿐이므로, 어떤 경우에는 그 해의 수확량 $y$가 예측된 신뢰구간 안에 들어오지 못하는 경우도 분명 있을 것입니다.  하지만 이 모델을 이용해 100번의 예측을 했다고 할 때, 우리가 잡은 신뢰구간의 $p$값인 0.9의 비율에 해당하는 90번 정도는 실제 수확량이 90% 신뢰구간 내에 들어와 주기를 바라는 정도는 통계적으로 그리 무리가 되는 요구는 아닐 것입니다.

이것이 베이지언 딥러닝 모델로부터 얻어지는 출력 분포로부터 신뢰구간을 계산하고 이를 활용하는 데 있어 우리가 가질 수 있는 기본적이고 직관적인 기대이며, 모든 $p$에 대해 데이터의 개수 $T$가 충분히 크다면 이러한 기대(**식 1**)를 만족하게 되는 예측기 $H$를 **잘 교정된 예측기**라고 한다는 것이 앞 절의 내용입니다. 

## 신뢰구간 보정의 필요성

모든 베이지언 예측기가 잘 교정된 예측기가 되는 것은 아닙니다.  가장 간단한 예시로, 단순히 실측되는 종속변수 $y$가 정규분포를 따르지 않기만 하더라도 베이지언 예측기는 교정되지 못한 예측기가 됩니다.  이런 경우 아무리 각 예측의 신뢰구간 밴드를 예쁘게 그려 놓더라도, 그 신뢰구간에 대해 우리가 기대하는 직관적인 예측은 전혀 충족되지 않으며, 신뢰구간을 그리는 데 사용된 $p$값의 크기와 관계 없이 $y$(실제 수확량)는 무작위적인 위치에서 발견되는 것처럼 보이게 됩니다.

Kuleshov와 Fenner, Ermon은 논문에서 이러한 합리적 기대를 충족하기 위한 보조 모델 $R$을 구성하는 방법을 제안합니다.  방법론은 다음과 같습니다.

우리가 가지고 있는 데이터포인트 집합을 $\mathcal{S}=\lbrace{(x_t, y_y)}\rbrace _{t=1}^T$ 라고 할 때, 어떤 스텝의 데이터에 대해서($t$) 예측기 $H$는 평균과 표준편차$(m_x,\sigma_x)$로 대표할 수 있는 예측 $H(x_t)$를 내놓을 것입니다.

이 모델은 보정되지 않았으므로, $H$에 의한 예측은 우리의 기대를 저버릴 수 있습니다.  따라서, 확률 $p$에 대한 신뢰구간을 그릴 때, 곧이곧대로 $p$에 대한 신뢰구간을 그려 주는 것이 아니라, 새로운 확률 $p$'에 대한 신뢰구간을 그려 줍니다.  보조모델 $R$은 $p$를 $p$'로 사상하는 역할을 합니다.

이 때 새로운 확률 $p$'는 앞서 설명한 **농부의 기대**에 부응할 수 있는 수치여야 하며.  이것을 구하는 방법은 아래와 같습니다.

$$\^{P}(p)= \lvert\lbrace y_t\vert\lbrack H(x_t)\rbrack\le p, t=1,...,T\rbrace\rvert/T \tag{2}$$

위의 수식은 다음과 같이 설명할 수 있습니다:  $R$은 확률 $p$를 새로운 확률 $\^{P}(p)$로 사상하는데, 이 때 $\^{P}(p)$는 내가 가진 데이터셋 $\mathcal{S}$의 $T$개 원소들 중 예측기 $H$가 내놓은 예측 $H(x_t)$로부터 계산된 확률 $p$에 대한 신뢰구간이 실측값 $y_t$를 포함하는 데 성공한 것들의 갯수의 비율입니다.

## 보조 모델의 학습

교정되지 않은(uncalibrated) 예측기 $H$가 **농부의 기대**를 만족하는 잘 교정된 예측기가 될 수 있도록 하는 **재교정 모델(recalibration model)** $R$은 데이터셋 $\mathcal{S}$로부터 재구성되는 새로운 데이터셋 $\mathcal{D}$를 이용하여 학습됩니다.

$$\mathcal{D}=\lbrace\lparen\lbrack H(x_t)\rbrack(y_t),\^{P}(\lbrack H(x_t)\rbrack(y_t))\rparen\rbrace _{t=1}^T \tag{2}$$

$\lbrack H(x_t)\rbrack(y_t)$는 예측된 분포 $H(x_t)$에서 구간 $\lbrack -\infty, y_t\rbrack$을 적분하여 얻은 확률입니다.  이것은 실측된 종속변수 $y_t$를 상한으로 하여 계산된 것이므로, 농부의 기대를 충족하지 않을 수 있는 가능성이 있는 값입니다.  이 값이 재교정 모델 $R$의 학습에 사용될 입력($x$)이 됩니다.

한편 $\^{P}(\lbrack(H(x_t)\rbrack(y_t))$는 농부의 기대를 만족하지 못하는 앞선 입력을 농부의 기대를 충족하는 값으로 바꾸어 준 값이며, $R$의 학습에 사용될 출력 혹은 정답 레이블($y$)이 됩니다.

$R$은 $\^{P}$의 정의로부터 단조증가한다는 것을 쉽게 알 수 있고, 등장회귀(Isotonic Regression) 등의 방법에 의해 모델링될 수 있습니다.  $\mathcal{D}$에 등장회귀법을 적용하여 얻은 $R$에 $F_t$를 합성한 $R\circ F_t$는 $R$의 정의로부터 잘 교정된 예측기의 성질을 만족한다는 것을 알 수 있습니다.

